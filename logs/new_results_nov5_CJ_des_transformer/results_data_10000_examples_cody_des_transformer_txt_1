    training_args = TrainingArguments(
    output_dir="output/",
    remove_unused_columns=False,
    num_train_epochs=120,
    per_device_train_batch_size=64,
    learning_rate=1e-4,
    weight_decay=1e-4,
    warmup_ratio=0.1,
    optim="adamw_torch",
    max_grad_norm=0.25,
)
    
    return_tensors: str = "pt"
    max_len: int = 20 #subsets of the episode we use for training
    state_dim: int = 25  # size of state space
    act_dim: int = 4  # size of action space
    max_ep_len: int = 50 # max episode length in the dataset
    scale: float = 1000.0  # normalization of rewards/returns
    state_mean: np.array = None  # to store state means
    state_std: np.array = None  # to store state stds
    p_sample: np.array = None  # a distribution to take account trajectory lengths
    n_traj: int = 0 # to store the number of trajectories in the dataset




DatasetDict({
    train: Dataset({
        features: ['observations', 'actions', 'rewards', 'dones'],
        num_rows: 10000
    })
})
1
10000
observations:  25

***** Running training *****
  Num examples = 10000
  Num Epochs = 120
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 18840
  Number of trainable parameters = 1258910

{'loss': 0.1759, 'learning_rate': 2.653927813163482e-05, 'epoch': 3.18}         
  3%|▉                                    | 500/18840 [02:34<1:41:20,  3.02it/s] 500

{'loss': 0.0623, 'learning_rate': 5.307855626326964e-05, 'epoch': 6.37}         
  5%|█▉                                  | 1000/18840 [05:14<1:33:50,  3.17it/s] 1000

{'loss': 0.0485, 'learning_rate': 7.961783439490447e-05, 'epoch': 9.55}         
  8%|██▊                                 | 1500/18840 [07:57<1:32:12,  3.13it/s] 1500

{'loss': 0.0422, 'learning_rate': 9.931587638594009e-05, 'epoch': 12.74}        
 11%|███▊                                | 2000/18840 [11:08<1:31:07,  3.08it/s] 2000

{'loss': 0.037, 'learning_rate': 9.636706770464733e-05, 'epoch': 15.92}         
 13%|████▊                               | 2500/18840 [14:27<1:49:31,  2.49it/s] 2500

{'loss': 0.034, 'learning_rate': 9.341825902335457e-05, 'epoch': 19.11}         
 16%|█████▋                              | 3000/18840 [17:27<1:23:46,  3.15it/s] 3000

{'loss': 0.032, 'learning_rate': 9.046945034206181e-05, 'epoch': 22.29}         
 19%|██████▋                             | 3500/18840 [20:05<1:28:06,  2.90it/s] 3500

{'loss': 0.0314, 'learning_rate': 8.752064166076905e-05, 'epoch': 25.48}        
 21%|███████▋                            | 4000/18840 [22:40<1:15:49,  3.26it/s] 4000

{'loss': 0.0288, 'learning_rate': 8.457183297947629e-05, 'epoch': 28.66}        
 24%|████████▌                           | 4500/18840 [25:14<1:14:54,  3.19it/s] 4500

{'loss': 0.029, 'learning_rate': 8.162302429818354e-05, 'epoch': 31.85}         
 27%|█████████▌                          | 5000/18840 [27:49<1:10:38,  3.27it/s] 5000

{'loss': 0.027, 'learning_rate': 7.867421561689078e-05, 'epoch': 35.03}         
 29%|██████████▌                         | 5500/18840 [30:23<1:05:12,  3.41it/s] 5500

{'loss': 0.0265, 'learning_rate': 7.572540693559802e-05, 'epoch': 38.22}        
 32%|███████████▍                        | 6000/18840 [32:58<1:05:57,  3.24it/s] 6000

{'loss': 0.0256, 'learning_rate': 7.277659825430526e-05, 'epoch': 41.4}         
 35%|████████████▍                       | 6500/18840 [35:33<1:02:21,  3.30it/s] 6500

{'loss': 0.0242, 'learning_rate': 6.98277895730125e-05, 'epoch': 44.59}         
 37%|█████████████▍                      | 7000/18840 [38:06<1:05:13,  3.03it/s] 7000

 38%|█████████████▌                      | 7102/18840 [38:38<1:10:09,  2.79it/s]
{'loss': 0.0247, 'learning_rate': 6.687898089171974e-05, 'epoch': 47.77}        
 40%|███████████████▏                      | 7500/18840 [40:45<59:51,  3.16it/s] 7500

{'loss': 0.0239, 'learning_rate': 6.393017221042698e-05, 'epoch': 50.96}        
 42%|████████████████▏                     | 8000/18840 [43:23<56:14,  3.21it/s] 8000

{'loss': 0.023, 'learning_rate': 6.098136352913423e-05, 'epoch': 54.14}         
 45%|█████████████████▏                    | 8500/18840 [46:01<53:38,  3.21it/s] 8500

{'loss': 0.023, 'learning_rate': 5.803255484784147e-05, 'epoch': 57.32}         
 48%|██████████████████▏                   | 9000/18840 [48:39<56:50,  2.89it/s] 9000

{'loss': 0.0229, 'learning_rate': 5.508374616654872e-05, 'epoch': 60.51}        
 50%|███████████████████▏                  | 9500/18840 [51:17<49:35,  3.14it/s] 9500

{'loss': 0.0229, 'learning_rate': 5.213493748525596e-05, 'epoch': 63.69}        
 53%|███████████████████▋                 | 10000/18840 [53:56<45:56,  3.21it/s] 10000

{'loss': 0.0221, 'learning_rate': 4.91861288039632e-05, 'epoch': 66.88}         
 56%|████████████████████▌                | 10500/18840 [56:34<43:29,  3.20it/s] 10500

{'loss': 0.022, 'learning_rate': 4.623732012267044e-05, 'epoch': 70.06}         
 58%|█████████████████████▌               | 11000/18840 [59:11<40:26,  3.23it/s] 11000

{'loss': 0.0218, 'learning_rate': 4.328851144137769e-05, 'epoch': 73.25}        
 61%|█████████████████████▎             | 11500/18840 [1:01:50<38:11,  3.20it/s] 11500

{'loss': 0.0216, 'learning_rate': 4.033970276008493e-05, 'epoch': 76.43}        
 64%|██████████████████████▎            | 12000/18840 [1:04:28<35:39,  3.20it/s] 12000

{'loss': 0.0217, 'learning_rate': 3.739089407879217e-05, 'epoch': 79.62}        
 66%|███████████████████████▏           | 12500/18840 [1:07:06<32:49,  3.22it/s] 12500

{'loss': 0.0208, 'learning_rate': 3.4442085397499415e-05, 'epoch': 82.8}        
 69%|████████████████████████▏          | 13000/18840 [1:09:44<32:53,  2.96it/s] 13000

{'loss': 0.0209, 'learning_rate': 3.1493276716206655e-05, 'epoch': 85.99}       
 72%|█████████████████████████          | 13500/18840 [1:12:22<28:00,  3.18it/s] 13500

{'loss': 0.0213, 'learning_rate': 2.8544468034913895e-05, 'epoch': 89.17}       
 74%|██████████████████████████         | 14000/18840 [1:15:01<27:43,  2.91it/s] 14000

{'loss': 0.0202, 'learning_rate': 2.559565935362114e-05, 'epoch': 92.36}        
 77%|██████████████████████████▉        | 14500/18840 [1:17:40<22:20,  3.24it/s] 14500

{'loss': 0.0203, 'learning_rate': 2.264685067232838e-05, 'epoch': 95.54}        
 80%|███████████████████████████▊       | 15000/18840 [1:20:17<19:51,  3.22it/s] 15000

{'loss': 0.0201, 'learning_rate': 1.9698041991035625e-05, 'epoch': 98.73}       
 82%|████████████████████████████▊      | 15500/18840 [1:23:17<19:37,  2.84it/s] 15500

{'loss': 0.0194, 'learning_rate': 1.6749233309742865e-05, 'epoch': 101.91}      
 85%|█████████████████████████████▋     | 16000/18840 [1:26:21<21:40,  2.18it/s] 16000

{'loss': 0.0198, 'learning_rate': 1.3800424628450107e-05, 'epoch': 105.1}       
 88%|██████████████████████████████▋    | 16500/18840 [1:29:32<14:30,  2.69it/s] 16500

{'loss': 0.02, 'learning_rate': 1.085161594715735e-05, 'epoch': 108.28}         
 90%|███████████████████████████████▌   | 17000/18840 [1:34:19<19:09,  1.60it/s] 17000

{'loss': 0.019, 'learning_rate': 7.902807265864591e-06, 'epoch': 111.46}        
 93%|████████████████████████████████▌  | 17500/18840 [1:39:25<13:00,  1.72it/s] 17500

{'loss': 0.0198, 'learning_rate': 4.953998584571833e-06, 'epoch': 114.65}       
 96%|█████████████████████████████████▍ | 18000/18840 [1:44:00<07:08,  1.96it/s] 18000

{'loss': 0.0199, 'learning_rate': 2.0051899032790755e-06, 'epoch': 117.83}      
 98%|██████████████████████████████████▎| 18500/18840 [1:48:17<02:53,  1.96it/s] 18500

100%|███████████████████████████████████| 18840/18840 [1:51:16<00:00,  2.53it/s]

Training completed.


{'train_runtime': 6676.7158, 'train_samples_per_second': 179.729, 'train_steps_per_second': 2.822, 'train_loss': 0.02995137932953561, 'epoch': 120.0}
100%|███████████████████████████████████| 18840/18840 [1:51:16<00:00,  2.82it/s]
pybullet build time: May 20 2022 19:44:17

